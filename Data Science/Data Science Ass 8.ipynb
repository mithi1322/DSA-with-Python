{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9768aaa5-1ff7-4c6d-bfa8-9b52fb3c419f",
   "metadata": {},
   "source": [
    "1. What is the difference between a neuron and a neural network?\n",
    "2. Can you explain the structure and components of a neuron?\n",
    "3. Describe the architecture and functioning of a perceptron.\n",
    "4. What is the main difference between a perceptron and a multilayer perceptron?\n",
    "5. Explain the concept of forward propagation in a neural network.\n",
    "6. What is backpropagation, and why is it important in neural network training?\n",
    "7. How does the chain rule relate to backpropagation in neural networks?\n",
    "8. What are loss functions, and what role do they play in neural networks?\n",
    "9. Can you give examples of different types of loss functions used in neural networks?\n",
    "10. Discuss the purpose and functioning of optimizers in neural networks.\n",
    "11. What is the exploding gradient problem, and how can it be mitigated?\n",
    "12. Explain the concept of the vanishing gradient problem and its impact on neural network training.\n",
    "13. How does regularization help in preventing overfitting in neural networks?\n",
    "14. Describe the concept of normalization in the context of neural networks.\n",
    "15. What are the commonly used activation functions in neural networks?\n",
    "16. Explain the concept of batch normalization and its advantages.\n",
    "17. Discuss the concept of weight initialization in neural networks and its importance.\n",
    "18. Can you explain the role of momentum in optimization algorithms for neural networks?\n",
    "19. What is the difference between L1 and L2 regularization in neural networks?\n",
    "20. How can early stopping be used as a regularization technique in neural networks?\n",
    "21. Describe the concept and application of dropout regularization in neural networks.\n",
    "22. Explain the importance of learning rate in training neural networks.\n",
    "23. What are the challenges associated with training deep neural networks?\n",
    "24. How does a convolutional neural network (CNN) differ from a regular neural network?\n",
    "25. Can you explain the purpose and functioning of pooling layers in CNNs?\n",
    "26. What is a recurrent neural network (RNN), and what are its applications?\n",
    "27. Describe the concept and benefits of long short-term memory (LSTM) networks.\n",
    "28. What are generative adversarial networks (GANs), and how do they work?\n",
    "29. Can you explain the purpose and functioning of autoencoder neural networks?\n",
    "30. Discuss the concept and applications of self-organizing maps (SOMs) in neural networks.\n",
    "31. How can neural networks be used for regression tasks?\n",
    "32. What are the challenges in training neural networks with large datasets?\n",
    "33. Explain the concept of transfer learning in neural networks and its benefits.\n",
    "34. How can neural networks be used for anomaly detection tasks?\n",
    "35. Discuss the concept of model interpretability in neural networks.\n",
    "36. What are the advantages and disadvantages of deep learning compared to traditional machine learning algorithms?\n",
    "37. Can you explain the concept of ensemble learning in the context of neural networks?\n",
    "38. How can neural networks be used for natural language processing (NLP) tasks?\n",
    "39. Discuss the concept and applications of self-supervised learning in neural networks.\n",
    "40. What are the challenges in training neural networks with imbalanced datasets?\n",
    "41. Explain the concept of adversarial attacks on neural networks and methods to mitigate them.\n",
    "42. Can you discuss the trade-off between model complexity and generalization performance in neural networks?\n",
    "43. What are some techniques for handling missing data in neural networks?\n",
    "44. Explain the concept and benefits of interpretability techniques like SHAP values and LIME in neural networks.\n",
    "45. How can neural networks be deployed on edge devices for real-time inference?\n",
    "46. Discuss the considerations and challenges in scaling neural network training on distributed systems.\n",
    "47. What are the ethical implications of using neural networks in decision-making systems?\n",
    "48. Can you explain the concept and applications of reinforcement learning in neural networks?\n",
    "49. Discuss the impact of batch size in training neural networks.\n",
    "50. What are the current limitations of neural networks and areas for future research?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7205dbc-c89a-4457-930f-3b03ff8ba6ad",
   "metadata": {},
   "source": [
    "# Answer 1:\n",
    "Neuron: \n",
    "A neuron is a fundamental building block of a neural network.It is an information-processing unit that receives input, performs a computation, and produces an output.\n",
    "\n",
    "Neural Network: \n",
    "A neural network is a collection of interconnected neurons organized in layers. It consists of an input layer, one or more hidden layers, and an output layer. Neural networks are designed to simulate the behavior of the human brain and are used for various machine learning tasks.\n",
    "\n",
    "# Answer 2: \n",
    "\n",
    "Structure and Components of a Neuron:\n",
    "\n",
    "Dendrites: Branch-like structures that receive signals from other neurons or external sources.\n",
    "\n",
    "Cell Body (Soma): Processes and integrates incoming signals from the dendrites.\n",
    "\n",
    "Axon: Transmits the integrated signal to other neurons or output systems.\n",
    "\n",
    "Synapses: Junctions between the axon terminals of one neuron and the dendrites of another neuron, where the transmission of signals occurs.\n",
    "\n",
    "# Answer 3: \n",
    "    \n",
    "Architecture and Functioning of a Perceptron:\n",
    "\n",
    "The perceptron is the simplest form of a neural network. It consists of a single artificial neuron with adjustable weights and a bias term.\n",
    "The inputs are multiplied by their corresponding weights, summed, and passed through an activation function.\n",
    "The activation function produces the output of the perceptron, which can be binary (e.g., 0 or 1) or continuous (e.g., between 0 and 1).\n",
    "The weights and bias are adjusted during training using a learning algorithm, such as the perceptron learning rule.\n",
    "\n",
    "# Answer 4: \n",
    "\n",
    "Difference between a Perceptron and a Multilayer Perceptron:\n",
    "\n",
    "Perceptron: A perceptron has a single layer and can only solve linearly separable problems. It is limited to binary classification tasks.\n",
    "\n",
    "Multilayer Perceptron (MLP): An MLP has one or more hidden layers between the input and output layers. It can solve nonlinear problems and handle complex tasks like regression and classification.\n",
    "\n",
    "# Answer 5: \n",
    "\n",
    "Forward Propagation in a Neural Network:\n",
    "\n",
    "Forward propagation is the process of computing the outputs of a neural network given an input.\n",
    "The inputs are propagated through the network layer by layer. Each layer performs a weighted sum of the inputs, applies an activation function, and passes the output to the next layer.\n",
    "The process continues until the output layer is reached, producing the final predictions or outputs of the network.\n",
    "\n",
    "# Answer 6: \n",
    "    \n",
    "Backpropagation in Neural Network Training:\n",
    "\n",
    "Backpropagation is an algorithm used to train neural networks by updating the network weights based on the difference between the predicted outputs and the desired outputs.\n",
    "It involves computing the gradients of the network's error with respect to the weights and propagating these gradients backward through the network.\n",
    "The gradients are used to update the weights using an optimization algorithm, such as gradient descent, to minimize the error and improve the network's performance.\n",
    "\n",
    "# Answer 7: \n",
    "    \n",
    "Chain Rule and Backpropagation:\n",
    "\n",
    "The chain rule is a fundamental rule of calculus used in backpropagation to compute the gradients of the network's error with respect to the weights.\n",
    "It allows the gradients to be calculated layer by layer by multiplying the gradients from the subsequent layers with the local gradients of each layer's activation function.\n",
    "By iteratively applying the chain rule, the gradients are efficiently propagated backward through the network during backpropagation.\n",
    "\n",
    "# Answer 8: \n",
    "    \n",
    "Loss Functions in Neural Networks:\n",
    "\n",
    "Loss functions measure the discrepancy between the predicted outputs of a neural network and the true outputs or labels.\n",
    "They play a crucial role in training neural networks by providing a quantifiable measure of the network's performance.\n",
    "The goal is to minimize the loss function during training to improve the accuracy of the network's predictions.\n",
    "\n",
    "# Answer 9: \n",
    "    \n",
    "Examples of Loss Functions in Neural Networks:\n",
    "\n",
    "Mean Squared Error (MSE): Used for regression tasks to measure the average squared difference between predicted and true values.\n",
    "\n",
    "Binary Cross-Entropy: Used for binary classification tasks to measure the difference between predicted probabilities and true labels.\n",
    "\n",
    "Categorical Cross-Entropy: Used for multiclass classification tasks to measure the difference between predicted class probabilities and true labels.\n",
    "\n",
    "# Answer 10: \n",
    "    \n",
    "Optimizers in Neural Networks:\n",
    "\n",
    "Optimizers are algorithms used to adjust the network's weights during training to minimize the loss function and improve the network's performance.\n",
    "They use techniques such as gradient descent and its variations to find the optimal set of weights that minimize the loss.\n",
    "Examples of optimizers include Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad.\n",
    "\n",
    "# Answer 11: \n",
    "    \n",
    "Exploding Gradient Problem:\n",
    "\n",
    "The exploding gradient problem occurs when the gradients in a neural network become very large during backpropagation.\n",
    "This can lead to unstable training, making it difficult for the network to converge.\n",
    "It can be mitigated by techniques such as gradient clipping, which limits the gradient values to a predefined threshold.\n",
    "\n",
    "# Answer 12: \n",
    "    \n",
    "Vanishing Gradient Problem:\n",
    "\n",
    "The vanishing gradient problem occurs when the gradients in a neural network become very small during backpropagation.\n",
    "This can hinder the training of deep networks as the gradients diminish rapidly with each layer, leading to slow convergence or no learning.\n",
    "Architectural changes, such as using activation functions like ReLU, can alleviate the vanishing gradient problem.\n",
    "\n",
    "# Answer 13: \n",
    "    \n",
    "Regularization in Preventing Overfitting:\n",
    "\n",
    "Regularization techniques are used to prevent overfitting, where the model performs well on training data but poorly on new, unseen data.\n",
    "Regularization adds a penalty term to the loss function to discourage complex or over-parameterized models.\n",
    "Examples of regularization techniques include L1 and L2 regularization, dropout, and early stopping.\n",
    "\n",
    "# Answer 14: \n",
    "    \n",
    "Normalization in Neural Networks:\n",
    "\n",
    "Normalization is the process of scaling input data to a standard range, usually between 0 and 1 or -1 and 1.\n",
    "It helps in stabilizing the training process, improving convergence, and preventing the dominance of certain input features over others.\n",
    "Common normalization techniques include feature scaling, such as min-max scaling or standardization.\n",
    "\n",
    "# Answer 15: \n",
    "    \n",
    "Commonly Used Activation Functions:\n",
    "\n",
    "Sigmoid: Maps the input to a sigmoid-shaped curve between 0 and 1, suitable for binary classification problems or when dealing with probabilities.\n",
    "ReLU (Rectified Linear Unit): Sets negative values to zero and keeps positive values unchanged, widely used in deep learning due to its simplicity and effectiveness.\n",
    "Tanh (Hyperbolic Tangent): Similar to the sigmoid function but maps the input to the range [-1, 1].\n",
    "Softmax: Used in multiclass classification problems to convert a vector of values into probabilities that sum up to 1.\n",
    "\n",
    "# Answer 16: \n",
    "    \n",
    "Batch Normalization:\n",
    "\n",
    "Batch normalization is a technique used to improve the training stability and performance of neural networks.\n",
    "It normalizes the outputs of a layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "Batch normalization helps mitigate the problem of internal covariate shift and allows the network to converge faster.\n",
    "\n",
    "# Answer 17: \n",
    "    \n",
    "Weight Initialization:\n",
    "\n",
    "Weight initialization refers to the process of setting the initial values of the network's weights.\n",
    "Proper weight initialization is crucial for network convergence and preventing issues such as vanishing or exploding gradients.\n",
    "Techniques like Xavier/Glorot initialization and He initialization are commonly used to initialize weights based on the size of the network's inputs and outputs.\n",
    "\n",
    "# Answer 18: \n",
    "    \n",
    "Momentum in Optimization Algorithms:\n",
    "\n",
    "Momentum is a technique used in optimization algorithms to accelerate convergence and overcome local optima.\n",
    "It introduces an additional term that accumulates a fraction of the previous weight updates, allowing the optimizer to continue moving in the direction of previous updates.\n",
    "Momentum helps the optimizer to traverse flatter regions of the loss surface and reach the global optima more efficiently.\n",
    "\n",
    "# Answer 19: \n",
    "    \n",
    "L1 and L2 Regularization:\n",
    "\n",
    "L1 regularization (Lasso regularization) adds the sum of the absolute values of the weights to the loss function, promoting sparsity and feature selection.\n",
    "L2 regularization (Ridge regularization) adds the sum of the squared weights to the loss function, encouraging smaller weights and reducing the impact of individual features.\n",
    "\n",
    "# Answer 20: \n",
    "    \n",
    "Early Stopping as a Regularization Technique:\n",
    "\n",
    "Early stopping is a regularization technique that halts the training process when the performance on a validation set starts to degrade.\n",
    "It prevents overfitting by stopping the training before the model starts to memorize the training data, ensuring better generalization to unseen data.\n",
    "\n",
    "# Answer 21: \n",
    "    \n",
    "Dropout Regularization:\n",
    "\n",
    "Dropout is a regularization technique that randomly sets a fraction of the output activations of a layer to zero during training.\n",
    "It helps prevent overfitting by reducing co-adaptation of neurons and encourages the network to learn more robust and generalizable representations.\n",
    "\n",
    "# Answer 22: \n",
    "    \n",
    "Learning Rate in Neural Networks:\n",
    "\n",
    "The learning rate determines the step size at which the optimizer adjusts the weights during training.\n",
    "A high learning rate may result in overshooting the optimal weights, while a low learning rate may lead to slow convergence or getting stuck in local optima.\n",
    "The learning rate needs to be carefully tuned to ensure efficient training and convergence.\n",
    "\n",
    "# Answer 23: \n",
    "    \n",
    "Challenges in Training Deep Neural Networks:\n",
    "\n",
    "Vanishing and exploding gradients: Gradient propagation becomes challenging in deep networks due to diminishing or exploding gradients, leading to difficulties in training.\n",
    "\n",
    "Computational complexity: Deep networks with many layers and parameters require significant computational resources for training.\n",
    "\n",
    "Overfitting: Deep networks with a large number of parameters are prone to overfitting and may require regularization techniques to generalize well.\n",
    "\n",
    "# Answer 24: \n",
    "    \n",
    "Convolutional Neural Network (CNN) vs. Regular Neural Network:\n",
    "\n",
    "CNN: Designed for processing grid-like input data such as images, using convolutional layers to extract local features and pooling layers to downsample and summarize information.\n",
    "Regular Neural Network: Also known as a fully connected neural network, processes sequential input data and learns complex patterns through the interconnected layers.\n",
    "\n",
    "# Answer 25:\n",
    "    \n",
    "Pooling Layers in CNNs:\n",
    "\n",
    "Pooling layers are used in CNNs to reduce the spatial dimensions of the feature maps while retaining important features.\n",
    "Common types of pooling include max pooling, which takes the maximum value in each pooling region, and average pooling, which calculates the average value.\n",
    "\n",
    "# Answer 26: \n",
    "    \n",
    "Recurrent Neural Network (RNN):\n",
    "\n",
    "RNNs are designed to handle sequential data by introducing connections between units that form directed cycles, allowing information to persist.\n",
    "RNNs have internal memory, enabling them to capture temporal dependencies and process sequences of variable lengths.\n",
    "They are used in applications such as natural language processing, speech recognition, and time series analysis.\n",
    "\n",
    "# Answer 27:\n",
    "    \n",
    "Long Short-Term Memory (LSTM) Networks:\n",
    "\n",
    "LSTM networks are a type of RNN that address the vanishing gradient problem and can capture long-term dependencies.\n",
    "They introduce memory cells and gating mechanisms that control the flow of information, enabling the network to learn and remember information over longer sequences.\n",
    "\n",
    "# Answer 28:\n",
    "    \n",
    "Generative Adversarial Networks (GANs):\n",
    "\n",
    "GANs consist of two components: a generator network and a discriminator network.\n",
    "The generator network generates synthetic data samples, while the discriminator network tries to distinguish between real and fake samples.\n",
    "Through an adversarial training process, GANs learn to generate realistic data samples.\n",
    "\n",
    "# Answer 29:\n",
    "    \n",
    "Autoencoder Neural Networks:\n",
    "\n",
    "Autoencoders are unsupervised learning models used for data compression and feature learning.\n",
    "They consist of an encoder network that maps the input data to a lower-dimensional representation and a decoder network that reconstructs the original input from the lower-dimensional representation.\n",
    "\n",
    "# Answer 30:\n",
    "    \n",
    "Self-Organizing Maps (SOMs):\n",
    "\n",
    "SOMs, also known as Kohonen maps, are unsupervised learning models used for clustering and visualizing high-dimensional data.\n",
    "They organize data points in a lower-dimensional grid while preserving the topological relationships of the input data.\n",
    "\n",
    "# Answer 31: \n",
    "    \n",
    "Neural Networks for Regression Tasks:\n",
    "\n",
    "Neural networks can be used for regression tasks by adjusting the network architecture and using appropriate loss functions.\n",
    "The output layer typically consists of a single neuron with a linear activation function to produce continuous predictions.\n",
    "\n",
    "# Answer 32: \n",
    "    \n",
    "Challenges in Training Neural Networks with Large Datasets:\n",
    "\n",
    "Memory requirements: Large datasets may not fit entirely in memory, requiring techniques such as batch training or data generators.\n",
    "\n",
    "Computational resources: Training neural networks with large datasets may require powerful hardware or distributed computing systems.\n",
    "\n",
    "Training time: Training on large datasets can be time-consuming, requiring efficient optimization algorithms and parallel computing techniques.\n",
    "\n",
    "# Answer 33:\n",
    "    \n",
    "Transfer Learning in Neural Networks:\n",
    "\n",
    "Transfer learning involves leveraging pre-trained models on one task or dataset and transferring their knowledge to a different but related task or dataset.\n",
    "It allows models to benefit from learned representations and speeds up training on new tasks with limited data.\n",
    "\n",
    "# Answer 34: \n",
    "    \n",
    "Neural Networks for Anomaly Detection:\n",
    "\n",
    "Neural networks can be used for anomaly detection by training them on normal data and identifying deviations from the learned patterns as anomalies.\n",
    "Autoencoders and generative models are commonly used for anomaly detection tasks.\n",
    "\n",
    "# Answer 35:\n",
    "\n",
    "Model Interpretability in Neural Networks:\n",
    "\n",
    "Model interpretability refers to the ability to understand and explain the decisions made by a neural network.\n",
    "Techniques such as feature importance analysis, gradient-based methods, and attention mechanisms can provide insights into the inner workings of neural networks.\n",
    "\n",
    "# Answer 36:\n",
    "    \n",
    "Advantages and Disadvantages of Deep Learning compared to Traditional Machine Learning:\n",
    "\n",
    "Advantages: Deep learning excels in tasks involving large datasets, complex patterns, and high-dimensional data. It can automatically learn features from raw data, reducing the need for manual feature engineering.\n",
    "Disadvantages: Deep learning models often require a large amount of labeled data, substantial computational resources, and longer training times compared to traditional machine learning algorithms. They can also be less interpretable.\n",
    "\n",
    "# Answer 37:\n",
    "    \n",
    "Ensemble Learning in Neural Networks:\n",
    "\n",
    "Ensemble learning involves combining the predictions of multiple neural network models to improve performance.\n",
    "Techniques such as bagging, boosting, and stacking can be applied to neural networks to create ensembles that are more robust and accurate than individual models.\n",
    "\n",
    "# Answer 38:\n",
    "    \n",
    "Neural Networks for Natural Language Processing (NLP):\n",
    "\n",
    "Neural networks have revolutionized NLP by enabling the development of models for tasks such as sentiment analysis, machine translation, text generation, and named entity recognition.\n",
    "Recurrent neural networks (RNNs) and transformer models, such as the Transformer architecture, have achieved state-of-the-art results in many NLP tasks.\n",
    "\n",
    "# Answer 39:\n",
    "    \n",
    "Self-Supervised Learning in Neural Networks:\n",
    "\n",
    "Self-supervised learning is an approach where neural networks learn representations from unlabeled data by solving pretext tasks.\n",
    "These learned representations can then be used for downstream supervised or unsupervised tasks, reducing the need for large labeled datasets.\n",
    "\n",
    "# Answer 40:\n",
    "    \n",
    "Challenges in Training Neural Networks with Imbalanced Datasets:\n",
    "\n",
    "Imbalanced datasets pose challenges as the network may become biased toward the majority class, leading to poor performance on minority classes.\n",
    "Techniques such as class weighting, oversampling, undersampling, and cost-sensitive learning can help address the imbalance and improve the network's performance.\n",
    "\n",
    "# Answer 41:\n",
    "    \n",
    "Adversarial Attacks on Neural Networks:\n",
    "\n",
    "Adversarial attacks involve intentionally manipulating input data to mislead neural networks and cause them to make incorrect predictions.\n",
    "Techniques such as adding imperceptible perturbations or crafting specific input patterns can fool neural networks.\n",
    "Defenses against adversarial attacks include adversarial training, input preprocessing, and detection techniques.\n",
    "\n",
    "# Answer 42:\n",
    "    \n",
    "Trade-off between Model Complexity and Generalization Performance:\n",
    "\n",
    "Increasing model complexity can improve performance on training data but may lead to overfitting and poor generalization to new data.\n",
    "Regularization techniques and model selection strategies help balance model complexity and generalization performance.\n",
    "\n",
    "# Answer 43:\n",
    "    \n",
    "Handling Missing Data in Neural Networks:\n",
    "\n",
    "Missing data can be handled by imputation techniques such as mean imputation, regression imputation, or using specialized methods like autoencoders.\n",
    "Techniques like dropout can also provide a form of robustness to missing data by randomly dropping units during training.\n",
    "\n",
    "# Answer 44:\n",
    "    \n",
    "Interpretability Techniques in Neural Networks:\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values and LIME (Local Interpretable Model-agnostic Explanations) are techniques used to explain the predictions of neural networks by attributing the contribution of each input feature to the prediction.\n",
    "These techniques help understand which features are most influential in the network's decision-making process.\n",
    "\n",
    "# Answer 45: \n",
    "    \n",
    "Deploying Neural Networks on Edge Devices:\n",
    "\n",
    "Deploying neural networks on edge devices involves optimizing the model for resource-constrained environments, such as mobile devices or IoT devices.\n",
    "Techniques like model quantization, pruning, and architecture design can help reduce the model size and computational requirements while maintaining performance.\n",
    "\n",
    "# Answer 46: \n",
    "\n",
    "Scaling Neural Network Training on Distributed Systems:\n",
    "\n",
    "Training neural networks on distributed systems involves parallelizing the computations across multiple devices or machines.\n",
    "Techniques such as data parallelism and model parallelism can be used to distribute the training process and reduce the training time.\n",
    "\n",
    "# Answer 47:\n",
    "    \n",
    "Ethical Implications of Neural Networks in Decision-Making Systems:\n",
    "\n",
    "Neural networks in decision-making systems raise concerns related to fairness, transparency, bias, and accountability.\n",
    "It is important to ensure that neural networks are trained on unbiased and representative data and that their decisions are interpretable and explainable.\n",
    "\n",
    "# Answer 48:\n",
    "    \n",
    "Reinforcement Learning in Neural Networks:\n",
    "\n",
    "Reinforcement learning is a branch of machine learning where agents learn to take actions in an environment to maximize cumulative rewards.\n",
    "Neural networks, such as deep Q-networks (DQNs), are used as function approximators to learn policies or value functions in reinforcement learning tasks.\n",
    "\n",
    "# Answer 49:\n",
    "    \n",
    "Impact of Batch Size in Training Neural Networks:\n",
    "\n",
    "The batch size determines the number of samples processed before updating the weights during training.\n",
    "Larger batch sizes can provide more stable gradients but require more memory, while smaller batch sizes may result in noisy gradients and slower convergence.\n",
    "\n",
    "# Answer 50: \n",
    "    \n",
    "Current Limitations of Neural Networks and Future Research Areas:\n",
    "\n",
    "Neural networks still face challenges in explainability, robustness, handling small data, and adapting to dynamic environments.\n",
    "Future research areas include improving interpretability, developing more efficient architectures, addressing ethical concerns, and advancing unsupervised and self-supervised learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a9ce5-2c55-4f69-9477-609d3930eab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
